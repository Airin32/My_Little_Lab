{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention (keras.layers.Layer) :\n",
    "    def __init__ (self,d_k) :\n",
    "        super(SelfAttention,self).__init__()\n",
    "        self.dk = d_k \n",
    "    \n",
    "    def build (self,input_shape) :\n",
    "        f_dimention = input_shape[-1]\n",
    "        self.WQ = self.add_weight(shape=(f_dimention,self.dk),initializer=\"random_normal\",trainable=True)\n",
    "        self.WK = self.add_weight(shape=(f_dimention,self.dk),initializer=\"random_normal\",trainable=True)\n",
    "        self.WV = self.add_weight(shape=(f_dimention,self.dk),initializer=\"random_normal\",trainable=True)\n",
    "    \n",
    "    def call(self,x) :\n",
    "        Q = tf.matmul(x,self.WQ)\n",
    "        K = tf.matmul(x,self.WK)\n",
    "        V = tf.matmul(x,self.WV)\n",
    "        dk = tf.math.sqrt(tf.cast(self.dk,dtype=tf.float32))\n",
    "        attention = tf.matmul(Q,tf.transpose(K,perm=[0,2,1])) / dk \n",
    "        outputs = tf.matmul(attention,V)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.random.normal((2,5,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Attention = SelfAttention(d_k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = Attention(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20,), dtype=float32, numpy=\n",
       "array([-1.2761335 , -0.3343832 ,  0.41384995,  0.09875622, -0.11075204,\n",
       "       -1.6409185 , -0.24328028,  1.2874672 , -0.12970327,  0.44577038,\n",
       "        0.7368687 ,  1.3209614 , -1.6936523 ,  1.357806  , -0.21452764,\n",
       "       -1.6222739 , -0.33537555, -0.33465895,  0.29321498,  0.51347256],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_random () :\n",
    "    return [np.random.randint(0,10) for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [make_random() for x in range(1000)]\n",
    "test = [make_random() for x in range(1000)]\n",
    "x_test = np.array(test)\n",
    "x_train = np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array([make_random() for _ in range(1000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[6, 7, 8, 5],\n",
       "        [4, 3, 9, 0],\n",
       "        [7, 4, 4, 3],\n",
       "        ...,\n",
       "        [0, 2, 1, 0],\n",
       "        [1, 0, 9, 3],\n",
       "        [4, 8, 8, 1]]),\n",
       " array([[1, 2, 2, 4],\n",
       "        [0, 6, 9, 0],\n",
       "        [4, 4, 7, 7],\n",
       "        ...,\n",
       "        [7, 3, 5, 6],\n",
       "        [7, 7, 2, 1],\n",
       "        [0, 5, 2, 6]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train,y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformers_Encoder (keras.layers.Layer) :\n",
    "    def __init__ (self,vocab_size,d_model,d_k,ffn,outputs_dim) :  \n",
    "        super (Transformers_Encoder,self).__init__()\n",
    "        self.Self_attention = SelfAttention(d_k=d_k)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                keras.layers.Dense(ffn,activation='relu'),\n",
    "                keras.layers.Dense(d_model)\n",
    "            ]\n",
    "        )\n",
    "        self.Embedding = keras.layers.Embedding(input_dim=vocab_size,output_dim=outputs_dim)\n",
    "        self.LSTM1 = keras.layers.LSTM(32,return_state= True)\n",
    "    \n",
    "    def call(self,x) :\n",
    "        x = self.Embedding(x)\n",
    "        x = self.Self_attention(x)\n",
    "        x = self.ffn(x)\n",
    "        _,state_h,state_c = self.LSTM1(x)\n",
    "        return state_h,state_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformers_Decoder (keras.layers.Layer) :\n",
    "    def __init__ (self,num_class,embbedding_dim) :\n",
    "        super (Transformers_Decoder,self).__init__()\n",
    "        self.Embedding = keras.layers.Embedding(input_dim=num_class,output_dim=embbedding_dim)\n",
    "        self.Long_sort_Term_memories = keras.layers.LSTM(32,return_sequences=True,return_state=True)\n",
    "        self.Outputs_dense = keras.layers.Dense(num_class,activation='softmax')\n",
    "    \n",
    "    def call(self,decoder_input,state_h,state_c) :\n",
    "        decoder_input = self.Embedding(decoder_input)\n",
    "        states = [state_h,state_c]\n",
    "        outputs,state_h,state_c = self.Long_sort_Term_memories(decoder_input,initial_state = states)\n",
    "        outputs = self.Outputs_dense(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformers_seq2seq (keras.Model) :\n",
    "    def __init__ (self,vocab_size,d_model,d_k,ffn,outputs_dim,num_class) :\n",
    "        super(Transformers_seq2seq,self).__init__()\n",
    "        self.Encoder = Transformers_Encoder(vocab_size=vocab_size,\n",
    "                                            d_model=d_model,\n",
    "                                            outputs_dim=outputs_dim,d_k=d_k,\n",
    "                                            ffn=ffn)\n",
    "        self.Decoder = Transformers_Decoder(num_class=num_class,embbedding_dim=outputs_dim)\n",
    "    \n",
    "    def call(self,x) :\n",
    "        encoder_input,decoder_input = x\n",
    "        state_h,state_c = self.Encoder(encoder_input)\n",
    "        outputs = self.Decoder(decoder_input,state_h,state_c)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Bagaimana cara belajar AI?',\n",
       "  'Ceritakan lelucon!',\n",
       "  'Ceritakan lelucon!',\n",
       "  'Apa makanan favoritmu?',\n",
       "  'Apa kabar?'],\n",
       " ['<SOS> Belajarlah teori dan praktekkan dengan kode!',\n",
       "  '<SOS> Baiklah, ini lelucon: Kenapa komputer tidak bisa tidur? Karena selalu ada bug!',\n",
       "  '<SOS> Baiklah, ini lelucon: Kenapa komputer tidak bisa tidur? Karena selalu ada bug!',\n",
       "  '<SOS> Aku tidak makan, tapi aku suka data!',\n",
       "  '<SOS> Aku baik.'],\n",
       " ['Belajarlah teori dan praktekkan dengan kode! <EOS>',\n",
       "  'Baiklah, ini lelucon: Kenapa komputer tidak bisa tidur? Karena selalu ada bug! <EOS>',\n",
       "  'Baiklah, ini lelucon: Kenapa komputer tidak bisa tidur? Karena selalu ada bug! <EOS>',\n",
       "  'Aku tidak makan, tapi aku suka data! <EOS>',\n",
       "  'Aku baik. <EOS>'])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Contoh kecil dataset chatbot, nanti diperbesar\n",
    "encoder_inputs = [\n",
    "    \"Apa kabar?\", \"Siapa namamu?\", \"Ceritakan sesuatu!\", \n",
    "    \"Dimana kamu tinggal?\", \"Siapa yang menciptakanmu?\", \"Bagaimana cuaca hari ini?\",\n",
    "    \"Ceritakan lelucon!\", \"Apa makanan favoritmu?\", \"Apa yang kamu suka lakukan?\", \n",
    "    \"Bagaimana cara belajar AI?\"\n",
    "]\n",
    "\n",
    "decoder_outputs = [\n",
    "    \"Aku baik.\", \"Aku AI.\", \"Baiklah, aku akan bercerita.\",\n",
    "    \"Aku tinggal di dalam komputer.\", \"Aku diciptakan oleh seorang programmer.\", \"Aku tidak tahu, aku tidak bisa melihat.\",\n",
    "    \"Baiklah, ini lelucon: Kenapa komputer tidak bisa tidur? Karena selalu ada bug!\", \"Aku tidak makan, tapi aku suka data!\", \"Aku suka berbicara denganmu!\", \n",
    "    \"Belajarlah teori dan praktekkan dengan kode!\"\n",
    "]\n",
    "\n",
    "# Menambahkan token <SOS> dan <EOS>\n",
    "decoder_inputs = [\"<SOS> \" + sentence for sentence in decoder_outputs]\n",
    "decoder_outputs = [sentence + \" <EOS>\" for sentence in decoder_outputs]\n",
    "\n",
    "# Memperbanyak dataset secara otomatis dengan variasi pertanyaan & jawaban\n",
    "big_encoder_inputs = []\n",
    "big_decoder_inputs = []\n",
    "big_decoder_outputs = []\n",
    "\n",
    "for _ in range(5000):  # Membuat 5000 sampel\n",
    "    i = random.randint(0, len(encoder_inputs) - 1)\n",
    "    big_encoder_inputs.append(encoder_inputs[i])\n",
    "    big_decoder_inputs.append(decoder_inputs[i])\n",
    "    big_decoder_outputs.append(decoder_outputs[i])\n",
    "\n",
    "# Menampilkan contoh kecil dari dataset besar\n",
    "big_encoder_inputs[:5], big_decoder_inputs[:5], big_decoder_outputs[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(encoder_inputs + decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_encoder = tokenizer.texts_to_sequences(big_encoder_inputs)\n",
    "X_train_decoder_inputs = tokenizer.texts_to_sequences(big_decoder_inputs)\n",
    "Y_train_decoder_outputs = tokenizer.texts_to_sequences(big_decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(seq) for seq in Y_train_decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_encoder = pad_sequences(x_train_encoder,maxlen=13,padding='post')\n",
    "X_train_decoder_inputs = pad_sequences(X_train_decoder_inputs,maxlen=13,padding='post')\n",
    "Y_train_decoder_outputs = pad_sequences(Y_train_decoder_outputs,maxlen=13,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat model\n",
    "vocab_size = len(tokenizer.word_index) + 1  # +1 untuk padding\n",
    "d_model = 64\n",
    "d_k = 32\n",
    "ffn_units = 128\n",
    "num_class = vocab_size  # Jumlah kata dalam tokenizer\n",
    "\n",
    "model = Transformers_seq2seq(vocab_size,d_model=d_model,d_k=d_k,ffn=ffn_units,outputs_dim=32,num_class=vocab_size)\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Latih model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformers_seq2seq_13\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"transformers_seq2seq_13\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ transformers__encoder_19        │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Transformers_Encoder</span>)          │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformers__decoder_16        │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Transformers_Decoder</span>)          │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ transformers__encoder_19        │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mTransformers_Encoder\u001b[0m)          │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformers__decoder_16        │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mTransformers_Decoder\u001b[0m)          │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.5261 - loss: 2.6321\n",
      "Epoch 2/5\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.7266 - loss: 1.0446\n",
      "Epoch 3/5\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.9920 - loss: 0.4299\n",
      "Epoch 4/5\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.1557\n",
      "Epoch 5/5\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0670\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1cf329c6ba0>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([x_train_encoder, X_train_decoder_inputs], Y_train_decoder_outputs, batch_size=32, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformers_seq2seq_13\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"transformers_seq2seq_13\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ transformers__encoder_19        │ ?                      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">29,920</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Transformers_Encoder</span>)          │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformers__decoder_16        │ ?                      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,090</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Transformers_Decoder</span>)          │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ transformers__encoder_19        │ ?                      │        \u001b[38;5;34m29,920\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformers_Encoder\u001b[0m)          │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformers__decoder_16        │ ?                      │        \u001b[38;5;34m12,090\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformers_Decoder\u001b[0m)          │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">126,032</span> (492.32 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m126,032\u001b[0m (492.32 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">42,010</span> (164.10 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m42,010\u001b[0m (164.10 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">84,022</span> (328.21 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m84,022\u001b[0m (328.21 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention (keras.layers.Layer) :\n",
    "    def __init__ (self,d_model,num_head) :\n",
    "        super (MultiHeadAttention,self).__init__()\n",
    "        self.num_head = num_head \n",
    "        self.d_k = d_model // num_head\n",
    "        self.d_model = d_model\n",
    "        self.W_q = keras.layers.Dense(d_model)\n",
    "        self.W_k = keras.layers.Dense(d_model)\n",
    "        self.W_v = keras.layers.Dense(d_model)\n",
    "        self.W_o = keras.layers.Dense(d_model)\n",
    "    \n",
    "    def scaled_dot_product (self,Q,K,V,mask = None ) :\n",
    "        dk = tf.cast(self.d_k,dtype=tf.float32)\n",
    "        scores = tf.matmul(Q,K,transpose_b=True) / tf.math.sqrt(dk)\n",
    "        if mask is not None  :\n",
    "            scores = tf.where(mask == 0,tf.constant(-1e9,dtype=scores.dtype),scores)\n",
    "        Attention_weight = tf.nn.softmax(scores,axis=-1)\n",
    "        outputs = tf.matmul(Attention_weight,V)\n",
    "        return outputs\n",
    "    \n",
    "    def split_heads (self,x,batch_size) :\n",
    "        x = tf.reshape(x,(batch_size,-1,self.num_head,self.d_k))\n",
    "        return tf.transpose(x,perm=[0,2,1,3])\n",
    "    \n",
    "    def call (self,Q,K,V,mask=None) :\n",
    "        batch_size = tf.shape(Q)[0]\n",
    "        Q = self.split_heads(Q,batch_size)\n",
    "        K = self.split_heads(K,batch_size)\n",
    "        V = self.split_heads(V,batch_size)\n",
    "        Attention = self.scaled_dot_product(Q,K,V,mask)\n",
    "        Attention = tf.transpose(Attention,perm=[0,2,1,3])\n",
    "        Attention = tf.reshape(Attention,(batch_size,-1,self.d_model))\n",
    "        outputs = self.W_o(Attention)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder (keras.layers.Layer) :\n",
    "    def __init__ (self,vocab_size,embedding_lenght,d_model,num_heads) :\n",
    "        super(Encoder,self).__init__()\n",
    "        self.Embedding = keras.layers.Embedding(input_dim=vocab_size,output_dim=embedding_lenght)\n",
    "        self.MultiHeadAttention = MultiHeadAttention(d_model,num_heads)\n",
    "        self.LSTM = keras.layers.Bidirectional(keras.layers.LSTM(64,return_sequences=True,return_state=True))\n",
    "        self.Normal1 = keras.layers.LayerNormalization()\n",
    "        self.ffn = keras.Sequential([\n",
    "            keras.layers.Dense(128,activation='relu'),\n",
    "            keras.layers.Dense(d_model)\n",
    "        ])\n",
    "        self.Outputs = keras.layers.Dense(d_model,activation='tanh')\n",
    "    \n",
    "    def call(self,x) :\n",
    "        x = self.Embedding(x)\n",
    "        x =self.MultiHeadAttention(x,x,x)\n",
    "        x = self.Normal1(x)\n",
    "        x = self.ffn(x)\n",
    "        outputs, forward_h, forward_c, backward_h, backward_c = self.LSTM(x)\n",
    "        encoded_outputs = self.Outputs(tf.concat([forward_h,backward_h],axis=-1))\n",
    "        return encoded_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder (keras.layers.Layer) :\n",
    "    def __init__ (self,num_class,vocab_size,d_model,num_head,outputs_dim) :\n",
    "        super(Decoder,self).__init__\n",
    "        self.Embedding = keras.layers.Embedding(input_dim=vocab_size,output_dim=outputs_dim)\n",
    "        self.MultiHeadAttention = MultiHeadAttention(d_model=d_model,num_head=num_head) \n",
    "        self.Biderectional_layers = keras.layers.Bidirectional(keras.layers.LSTM(64,return_sequences=True))\n",
    "        self.outputs_layers = keras.layers.Dense(num_class,activation='softmax')\n",
    "        self.normalistion = keras.layers.LayerNormalization()\n",
    "        self.ffn = keras.Sequential([\n",
    "            keras.layers.Dense(128,activation='relu'),\n",
    "            keras.layers.Dense(d_model)\n",
    "        ])\n",
    "    \n",
    "    def call(self,x) :\n",
    "        x = self.Embedding(x)\n",
    "        x = self.MultiHeadAttention(x,x,x)\n",
    "        x = self.Biderectional_layers(x)\n",
    "        x = self.normalistion(x)\n",
    "        x = self.ffn(x)\n",
    "        x = self.outputs_layers(x)\n",
    "        return x\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 6, 2, 6],\n",
       "       [4, 3, 0, 2],\n",
       "       [8, 0, 6, 5],\n",
       "       ...,\n",
       "       [0, 9, 6, 0],\n",
       "       [0, 1, 0, 8],\n",
       "       [3, 0, 7, 9]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 11\n",
    "embdedding = 64\n",
    "d_model = 128\n",
    "num_head = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train= np.array([make_random() for _ in range(1000)])\n",
    "y_train = x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(x_train)) :\n",
    "    random = np.random.randint(0,3) \n",
    "    y_train[i][random] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 8, 0, 6],\n",
       "       [0, 5, 6, 0],\n",
       "       [1, 0, 8, 8],\n",
       "       ...,\n",
       "       [5, 0, 5, 6],\n",
       "       [0, 8, 0, 1],\n",
       "       [9, 3, 0, 2]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array([make_random() for _ in range(1000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoders_Layers_Models = Encoder(vocab_size=vocab_size,embedding_lenght=embdedding,d_model = d_model,num_heads=num_head,seq_len=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Decoder_models = Decoder(embedding_length=embdedding,d_model=d_model,num_classes=vocab_size,num_heads=num_head,vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_decode = Encoders_Layers_Models(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1000, 128, 64), dtype=float32, numpy=\n",
       "array([[[ 0.04742417,  0.0298645 , -0.02125306, ...,  0.00365547,\n",
       "          0.0212527 , -0.01607012],\n",
       "        [ 0.04742417,  0.0298645 , -0.02125306, ...,  0.00365547,\n",
       "          0.0212527 , -0.01607012],\n",
       "        [ 0.04742417,  0.0298645 , -0.02125306, ...,  0.00365547,\n",
       "          0.0212527 , -0.01607012],\n",
       "        ...,\n",
       "        [ 0.04742417,  0.0298645 , -0.02125306, ...,  0.00365547,\n",
       "          0.0212527 , -0.01607012],\n",
       "        [ 0.04742417,  0.0298645 , -0.02125306, ...,  0.00365547,\n",
       "          0.0212527 , -0.01607012],\n",
       "        [ 0.04742417,  0.0298645 , -0.02125306, ...,  0.00365547,\n",
       "          0.0212527 , -0.01607012]],\n",
       "\n",
       "       [[ 0.04742417,  0.0298645 , -0.02125306, ...,  0.00365547,\n",
       "          0.0212527 , -0.01607012],\n",
       "        [ 0.04742417,  0.0298645 , -0.02125306, ...,  0.00365547,\n",
       "          0.0212527 , -0.01607012],\n",
       "        [ 0.04742417,  0.0298645 , -0.02125306, ...,  0.00365547,\n",
       "          0.0212527 , -0.01607012],\n",
       "        ...,\n",
       "        [ 0.04742417,  0.0298645 , -0.02125306, ...,  0.00365547,\n",
       "          0.0212527 , -0.01607012],\n",
       "        [ 0.04742417,  0.0298645 , -0.02125306, ...,  0.00365547,\n",
       "          0.0212527 , -0.01607012],\n",
       "        [ 0.04742417,  0.0298645 , -0.02125306, ...,  0.00365547,\n",
       "          0.0212527 , -0.01607012]],\n",
       "\n",
       "       [[ 0.04742417,  0.0298645 , -0.02125306, ...,  0.00365547,\n",
       "          0.0212527 , -0.01607012],\n",
       "        [ 0.04742417,  0.0298645 , -0.02125306, ...,  0.00365547,\n",
       "          0.0212527 , -0.01607012],\n",
       "        [ 0.04742417,  0.0298645 , -0.02125306, ...,  0.00365547,\n",
       "          0.0212527 , -0.01607012],\n",
       "        ...,\n",
       "        [ 0.04742417,  0.0298645 , -0.02125306, ...,  0.00365547,\n",
       "          0.0212527 , -0.01607012],\n",
       "        [ 0.04742417,  0.0298645 , -0.02125306, ...,  0.00365547,\n",
       "          0.0212527 , -0.01607012],\n",
       "        [ 0.04742417,  0.0298645 , -0.02125306, ...,  0.00365547,\n",
       "          0.0212527 , -0.01607012]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.04742417,  0.0298645 , -0.02125306, ...,  0.00365547,\n",
       "          0.0212527 , -0.01607012],\n",
       "        [ 0.04742417,  0.0298645 , -0.02125306, ...,  0.00365547,\n",
       "          0.0212527 , -0.01607012],\n",
       "        [ 0.04742417,  0.0298645 , -0.02125306, ...,  0.00365547,\n",
       "          0.0212527 , -0.01607012],\n",
       "        ...,\n",
       "        [ 0.04742417,  0.0298645 , -0.02125306, ...,  0.00365547,\n",
       "          0.0212527 , -0.01607012],\n",
       "        [ 0.04742417,  0.0298645 , -0.02125306, ...,  0.00365547,\n",
       "          0.0212527 , -0.01607012],\n",
       "        [ 0.04742417,  0.0298645 , -0.02125306, ...,  0.00365547,\n",
       "          0.0212527 , -0.01607012]],\n",
       "\n",
       "       [[ 0.04742417,  0.0298645 , -0.02125306, ...,  0.00365547,\n",
       "          0.0212527 , -0.01607012],\n",
       "        [ 0.04742417,  0.0298645 , -0.02125306, ...,  0.00365547,\n",
       "          0.0212527 , -0.01607012],\n",
       "        [ 0.04742417,  0.0298645 , -0.02125306, ...,  0.00365547,\n",
       "          0.0212527 , -0.01607012],\n",
       "        ...,\n",
       "        [ 0.04742417,  0.0298645 , -0.02125306, ...,  0.00365547,\n",
       "          0.0212527 , -0.01607012],\n",
       "        [ 0.04742417,  0.0298645 , -0.02125306, ...,  0.00365547,\n",
       "          0.0212527 , -0.01607012],\n",
       "        [ 0.04742417,  0.0298645 , -0.02125306, ...,  0.00365547,\n",
       "          0.0212527 , -0.01607012]],\n",
       "\n",
       "       [[ 0.04742417,  0.0298645 , -0.02125306, ...,  0.00365547,\n",
       "          0.0212527 , -0.01607012],\n",
       "        [ 0.04742417,  0.0298645 , -0.02125306, ...,  0.00365547,\n",
       "          0.0212527 , -0.01607012],\n",
       "        [ 0.04742417,  0.0298645 , -0.02125306, ...,  0.00365547,\n",
       "          0.0212527 , -0.01607012],\n",
       "        ...,\n",
       "        [ 0.04742417,  0.0298645 , -0.02125306, ...,  0.00365547,\n",
       "          0.0212527 , -0.01607012],\n",
       "        [ 0.04742417,  0.0298645 , -0.02125306, ...,  0.00365547,\n",
       "          0.0212527 , -0.01607012],\n",
       "        [ 0.04742417,  0.0298645 , -0.02125306, ...,  0.00365547,\n",
       "          0.0212527 , -0.01607012]]], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Embedding(input_dim=vocab_size,output_dim=64)(x_train_decode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
